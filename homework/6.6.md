1.  
> Пользователь (разработчик) написал в канал поддержки, что у него уже 3 минуты происходит CRUD-операция в MongoDB и её нужно прервать.  
> Вы как инженер поддержки решили произвести эту операцию:  
> - напишите список операций, которые вы будете производить для остановки запроса пользователя;
> - предложите вариант решения проблемы с долгими (зависающими) запросами в MongoDB.
  
С помощью списка операций определить искомую db.currentOp(). Можно дополнить параметром времени исполнения db.currentOp({"active": true, "secs_running": {"$gt": 180}}) (более трех минут). Выполнить db.killOp(id искомой операции), передать информацию об ошибке в фронтэнд. Можно изучать длинные операции с помощью включения профилировщика. В качестве превентивной меры можно выставить оператор $maxTimeMS, если доподлинно известно, что это не повлияет на качество работы с БД в формате часто выполняемых запросов.  
    
2.  
> Вы запустили инстанс Redis для использования совместно с сервисом, который использует механизм TTL. Причём отношение количества записанных key-value-значений к количеству истёкших значений есть величина постоянная и увеличивается пропорционально количеству реплик сервиса.  
> При масштабировании сервиса до N реплик вы увидели, что:  
> - сначала происходит рост отношения записанных значений к истекшим,
> - Redis блокирует операции записи.  
>   
> Как вы думаете, в чём может быть проблема?
  
Redis пишет помеченные на удаление ключи в оперативную память и не удаляет их до тех пор, пока не будет достигнуто значение остатка, равное 25% памяти. При превышении значения в 25% redis будет блокировать возможность записи новых ключей до момента возврата предельного значения обратно к <25%. В соответствии с тем, что эта механика потенциально предполагает обработку ключей адаптивным способом 10 раз в секунду по 20 помеченных на удаление ключей (по умолчанию), необходимость обработки более чем 200 ключей (а правильнее сказать 10 операций по 20 ключей) в секунду также может приводить к блокировке.  
В соответствии с рекомендациями:  
-	стоит выделять достаточное количество оперативной памяти под БД в кластере с учетом фактора вышеописанных алгоритмов очистки expired keys redis-ом;
-	не стоит пренебрегать мониторингом redis (Prometheus с Grafana, например);
-	стоит следить за тем, что лимит количества ключей, помеченных на удаление в конкретное время (EXPIREAT), не будет превышен;
-	не стоит использовать swap памяти.
  
3.  
> Вы подняли базу данных MySQL для использования в гис-системе. При росте количества записей в таблицах базы пользователи начали жаловаться на ошибки вида:  
```InterfaceError: (InterfaceError) 2013: Lost connection to MySQL server during query u'SELECT..... ' ```  
> Как вы думаете, почему это начало происходить и как локализовать проблему?  
> Какие пути решения этой проблемы вы можете предложить?  
  
Причина происходящего – увеличение размера БД MySQL, увеличение количества обращений сетевых пользователей-клиентов. В общих случаях рекомендуется увеличить  net_read_timeout или connect_timeout, в зависимости от первичной причины ошибки (тип запроса или проблемы соединения). Более тонкую настройку рекомендовано производить с параметрами wait_timeout, max_connection, thread_cache_size. Уменьшение первого параметра позволит снизить количество соединений с неактивными (idle) запросами. Увеличение max_connection позволит минимизировать риски невозможности подключения новыми клиентами в ситуации присутствия множества неактивных соединений. В свою очередь для многопоточного сервера MySQL стоит выставить thread_cache_size для определения потоков, сохраняемых в кэш для повторного использования. Увеличение значения этого параметра может положительно влиять на производительность БД в случае нарастания количества новых подключений.  
  
4.  
> Вы решили перевести гис-систему из задачи 3 на PostgreSQL, так как прочитали в документации, что эта СУБД работает с большим объёмом данных лучше, чем MySQL.  
> После запуска пользователи начали жаловаться, что СУБД время от времени становится недоступной. В dmesg вы видите, что:  
> ```postmaster invoked oom-killer```  
> Как вы думаете, что происходит?  
> Как бы вы решили эту проблему?  
  
Причина произошедшего – нехватка оперативной памяти для объемных процессов. OOM-killer убивает процесс, имеющий наибольшую оценку oom_score, приэтом процессы, связанные с привилегированным пользователем, имеют более низкую оценку и меньше шансов на принудительное завершение OOM-killer-ом.  
Тюнить OOM-killer можно в /etc/sysctl.conf:  
vm.overcommit_memory выставить 2 вместо 0 по умолчанию, передав ядру не резервировать больше памяти, чем указано в параметре overcommit_ratio.  
И, соответственно в overcommit_ratio выставить процент памяти, для которого допустимо избыточное резервирование.  
Если необходимо предотвратить принудительное завершение процесса postgres можно выставить /proc/(id postgres)/oom_score_adj отрицательным значением.  
