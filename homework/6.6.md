1.  
> Пользователь (разработчик) написал в канал поддержки, что у него уже 3 минуты происходит CRUD-операция в MongoDB и её нужно прервать.  
> Вы как инженер поддержки решили произвести эту операцию:  
> - напишите список операций, которые вы будете производить для остановки запроса пользователя;
> - предложите вариант решения проблемы с долгими (зависающими) запросами в MongoDB.
  
С помощью списка операций определить искомую db.currentOp(). Можно дополнить параметром времени исполнения db.currentOp({"active": true, "secs_running": {"$gt": 180}}) (более трех минут). Выполнить db.killOp(id искомой операции), передать информацию об ошибке в фронтэнд. Можно изучать длинные операции с помощью включения профилировщика. В качестве превентивной меры можно выставить оператор $maxTimeMS, если доподлинно известно, что это не повлияет на качество работы с БД в формате часто выполняемых запросов.  
    
2.  
> Вы запустили инстанс Redis для использования совместно с сервисом, который использует механизм TTL. Причём отношение количества записанных key-value-значений к количеству истёкших значений есть величина постоянная и увеличивается пропорционально количеству реплик сервиса.  
> При масштабировании сервиса до N реплик вы увидели, что:  
> - сначала происходит рост отношения записанных значений к истекшим,
> - Redis блокирует операции записи.  
>   
> Как вы думаете, в чём может быть проблема?
  
Redis пишет помеченные на удаление ключи в оперативную память и не удаляет их до тех пор, пока не будет достигнуто значение остатка, равное 25% памяти. При превышении значения в 25% redis будет блокировать возможность записи новых ключей до момента возврата предельного значения обратно к <25%. В соответствии с тем, что эта механика потенциально предполагает обработку ключей адаптивным способом 10 раз в секунду по 20 помеченных на удаление ключей (по умолчанию), необходимость обработки более чем 200 ключей (а правильнее сказать 10 операций по 20 ключей) в секунду также может приводить к блокировке.  
В соответствии с рекомендациями:  
-	стоит выделять достаточное количество оперативной памяти под БД в кластере с учетом фактора вышеописанных алгоритмов очистки expired keys redis-ом;
-	не стоит пренебрегать мониторингом redis (Prometheus с Grafana, например);
-	стоит следить за тем, что лимит количества ключей, помеченных на удаление в конкретное время (EXPIREAT), не будет превышен;
-	не стоит использовать swap памяти.
  
3.  
> Вы подняли базу данных MySQL для использования в гис-системе. При росте количества записей в таблицах базы пользователи начали жаловаться на ошибки вида:  
```InterfaceError: (InterfaceError) 2013: Lost connection to MySQL server during query u'SELECT..... ' ```  
> Как вы думаете, почему это начало происходить и как локализовать проблему?  
> Какие пути решения этой проблемы вы можете предложить?  
  
Причина происходящего – увеличение размера БД MySQL, увеличение количества обращений сетевых пользователей-клиентов. В общих случаях рекомендуется увеличить  net_read_timeout или connect_timeout, в зависимости от первичной причины ошибки (тип запроса или проблемы соединения). Более тонкую настройку рекомендовано производить с параметрами wait_timeout, max_connection, thread_cache_size. Уменьшение первого параметра позволит снизить количество соединений с неактивными (idle) запросами. Увеличение max_connection позволит минимизировать риски невозможности подключения новыми клиентами в ситуации присутствия множества неактивных соединений. В свою очередь для многопоточного сервера MySQL стоит выставить thread_cache_size для определения потоков, сохраняемых в кэш для повторного использования. Увеличение значения этого параметра может положительно влиять на производительность БД в случае нарастания количества новых подключений.  
  
4.  
> Вы решили перевести гис-систему из задачи 3 на PostgreSQL, так как прочитали в документации, что эта СУБД работает с большим объёмом данных лучше, чем MySQL.  
> После запуска пользователи начали жаловаться, что СУБД время от времени становится недоступной. В dmesg вы видите, что:  
> ```postmaster invoked oom-killer```  
> Как вы думаете, что происходит?  
> Как бы вы решили эту проблему?  
  
Причина произошедшего – нехватка оперативной памяти для объемных процессов. 
В соответствии с информацией о «никакой» конфигурации PostgreSQL «из коробки», можем внести как минимум следующие изменения в конфиг:
- увеличить shared_buffer до 25% RAM для увеличения буферной памяти
- увеличить wal_buffers для увеличения производительности при большом количестве одновременных подключений
- увеличить effective_cache_size до 75% RAM для того, чтобы PostgreSQL более эффективно осуществлял сканирование по  индексам при запросах
- увеличить work_mem при обработке объемных запросов, связанных с сортировкой или хеш-таблицами
- незначительно увеличить maintenance_work_mem для более эффективного выполнения внутренних задач обслуживания PostgreSQL (до 10%)
- для БД, в которых можно пренебречь согласованностью в пользу производительности можно выключить synchronous_commit
- изменить Temp_buffers в зависимости от величины потребности работы с временным буфером
и возможно осуществить более тонкую настройку в зависимости от задач СУБД
[Источник](https://cloud.yandex.ru/docs/managed-postgresql/concepts/settings-list)
Если необходимо предотвратить принудительное завершение процесса postgres можно выставить /proc/(id postgres)/oom_score_adj отрицательным значением.  
